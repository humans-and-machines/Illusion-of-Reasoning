\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.05}
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}} l r r r @{}}
\toprule
\textbf{Metric} & \textbf{Xword} & \textbf{Math} & \textbf{RHour} & \textbf{Combined} \\
\midrule
$N$ & 99{,}840 & 464{,}000 & 331{,}120 & 894{,}960 \\
$\beta_{\mathrm{ent}}$ & -0.033 & +0.019 & -0.407 & +0.011 \\
$\mathrm{OR}_{1\sigma}$ & 0.97 & 1.02 & 0.67 & 1.01 \\
$\mathrm{AME}$ (pp) & -0.23 & +0.24 & -0.01 & +0.08 \\
$p$ & 0.09108 & 0.1464 & 2.36\times10^{-119} & 0.397 \\
\bottomrule
\end{tabular*}
\caption{\textbf{Pass-2 accuracy rises with pass-1 entropy (Qwen2.5-1.5B).} We regress pass-2 correctness on standardized pass-1 entropy, controlling for pass-1 correctness and problem fixed effects. $\beta_{\mathrm{ent}}$ is the log-odds coefficient for a 1 SD entropy increase; $\mathrm{OR}_{1\sigma}=\exp(\beta_{\mathrm{ent}})$; and $\mathrm{AME}$ is the average marginal effect in percentage points. Combined rows re-standardize entropy using a pooled mean/SD across domains.}
\label{tab:pass2-entropy-regression}
\end{table}
