To test whether this pattern is specific to small GRPO-tuned models, we evaluate DeepSeek\textendash R1 and GPT\textendash 4o under matched decoding conditions on \textsc{MATH\textendash 500}. As shown in Table~\ref{tab:external-models}, both models exhibit \emph{very low} canonical shift rates across temperatures (0.40--0.60\% for DeepSeek\textendash R1 and 2.20--3.00\% for GPT\textendash 4o), and accuracy conditioned on a shift shows no systematic benefit, suggesting that the phenomenon generalizes across model families and training paradigms.
